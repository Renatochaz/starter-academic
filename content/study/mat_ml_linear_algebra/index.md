---
title: "Mathematics for Machine Learning: Linear Algebra"
summary: Resources and help in exercises for Linear Algebra
tags: 
- Mathematics for Machine Learning
- Linear Algebra
- Exercises 
date: "2021-01-14T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

image:
  caption: ""
  focal_point: Smart

links:
- icon: github
  icon_pack: fab
  name: Code
  url: https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---
**In no way graded assignments  and exams needed to certificate will be shared due to the Coursera honor code**

The aim of my repository is to give students learning linear algebra (in special those doing the Imperial College London Mathematics for Machine Learning course) some helpful resources and somewhere to guide then in the practice exercises available at the course. 

* The shareable code and notebooks are in the [github repository](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra) linked in this post. Feel free to access, comment, star or fork, interactions motivate me to produce and share more content for free, and suggestions to improvement are always viewed in a positive light! 



Available notes and exercises resolutions:

- [Week 1](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra/blob/master/week1.ipynb) 

  > Solving simultaneous equations

- [Week 2](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra/blob/master/Week2.ipynb)

  > Modulus e inner products
  >
  > Cosine e dot products
  >
  > Scalar and vector projections
  >
  > Basis change
  >
  > Linear dependence

- [Week 3](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra/blob/master/week3.ipynb) 

  > Matrix multiplication
  >
  > Matrix properties
  >
  > Identity matrix 
  >
  > Matrix transformation
  >
  > Solving simultaneous equations through matrix method
  >
  > Inverse matrix

- [Week 4](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra/blob/master/week4.ipynb) 

  > Einstein summation
  >
  > Symmetry of the dot product
  >
  > Notes on non-square matrix multiplication
  >
  > Changing basis in matrices
  >
  > Transformation in changed basis
  >
  > Orthogonal matrices
  >
  > Gram-schmidt process

- [Week 5](https://github.com/Renatochaz/mathematics_for_ML-Linear_algebra/blob/master/week5.ipynb)

  > Eigenvalues
  >
  > Eigenvectors
  >
  > Special eigen-cases
  >
  > Changing to the eigenbasis

**Basic course description (From Imperial College London course)**
*In this course on Linear Algebra we look at what linear algebra is and how it relates to vectors and matrices. Then we look through what vectors and matrices are and how to work with them, including the knotty problem of eigenvalues and eigenvectors, and how to use these to solve problems. Finally  we look at how to use these to do fun things with datasets - like how to rotate images of faces and how to extract eigenvectors to look at how the Pagerank algorithm works. Since we're aiming at data-driven applications, we'll be implementing some of these ideas in code, not just on pencil and paper. Towards the end of the course, you'll write code blocks and encounter Jupyter notebooks in Python, but don't worry, these will be quite short, focussed on the concepts, and will guide you through if youâ€™ve not coded before. At the end of this course you will have an intuitive understanding of vectors and matrices that will help you bridge the gap into linear algebra problems, and how to apply these concepts to machine learning.*

[Course link](https://www.coursera.org/learn/linear-algebra-machine-learning/home)







