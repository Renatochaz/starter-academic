---
title: "Mathematics for Machine Learning: Principal Component Analysis"
summary: Resources and help in exercises for PCA
tags: 
- Mathematics for Machine Learning
- PCA
- Principal Component Analysis 
date: "2021-02-12T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

image:
  caption: ""
  focal_point: Smart

links:
- icon: github
  icon_pack: fab
  name: Code
  url: https://github.com/Renatochaz/Mathematics_for_Machine_Learning
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---
**In no way graded assignments  and exams needed to certificate will be shared due to the Coursera honor code**

The aim of my repository is to give students learning Principal Component Analysis (PCA) (in special those doing the Imperial College London Mathematics for Machine Learning course) some helpful resources and somewhere to guide then in the practice exercises available at the course. 

* The shareable code and notebooks are in the [github repository](https://github.com/Renatochaz/Mathematics_for_Machine_Learning) linked in this post. Feel free to access, comment, star or fork, interactions motivate me to produce and share more content for free, and suggestions to improvement are always viewed in a positive light! 



Available notes:

# Principal Component Analysis

- [Week 1](https://github.com/Renatochaz/Mathematics_for_Machine_Learning/blob/master/pca_week1.ipynb):
  > Mean of a dataset;
  >
  > One dimensional variance;
  >
  > Covariance matrix;
  >
  > Linear transformation properties for the mean, variance and covariance;
  >
  > Numpy tutorial (from the course lab);
  >
  > A little gift for you, if you come this far :\)
- [Week 2](https://github.com/Renatochaz/Mathematics_for_Machine_Learning/blob/master/pca_week2.ipynb):
  > Dot product, angles and distance between vectors;
  >
  > Inner products;
  >
  > Inner products and length of vectors;
  >
  > Inner products, orthogonality and angle between vectors.
- [Week 3](https://github.com/Renatochaz/Mathematics_for_Machine_Learning/blob/master/pca_week3.ipynb):
  > Projections onto 1-D subspace;
  >
  > Projections in higher dimensions (N-D subspace).
- [Week 4](https://github.com/Renatochaz/Mathematics_for_Machine_Learning/blob/master/pca_week4.ipynb):
  > PCA objective and key ideas;
  >
  > Coordinates of projected data;
  >
  > Derivation of the average square reconstruction error;
  >
  > Finding the basis vectors that spans the principal subspace;
  >
  > Summary of key equations.
  > My personal note on how to create a PCA function (to help in the final assignment).


**Basic course description (From Imperial College London course)**This intermediate-level course *introduces the mathematical foundations to derive Principal Component Analysis (PCA), a fundamental dimensionality reduction technique. We'll cover some basic statistics of data sets, such as mean values and variances, we'll compute distances and angles between vectors using inner products and derive orthogonal projections of data onto lower-dimensional subspaces. Using all these tools, we'll then derive PCA as a method that minimizes the average squared reconstruction error between data points and their reconstruction.*

*At the end of this course, you'll be familiar with important mathematical concepts and you can implement PCA all by yourself. If youâ€™re struggling, you'll find a set of jupyter notebooks that will allow you to explore properties of the techniques and walk you through what you need to do to get on track. If you are already an expert, this course may refresh some of your knowledge. The lectures, examples and exercises require: 1. Some ability of abstract thinking 2. Good background in linear algebra (e.g., matrix and vector algebra, linear independence, basis) 3. Basic background in multivariate calculus (e.g., partial derivatives, basic optimization) 4. Basic knowledge in python programming and numpy Disclaimer: This course is substantially more abstract and requires more programming than the other two courses of the specialization. However, this type of abstract thinking, algebraic manipulation and programming is necessary if you want to understand and develop machine learning algorithms.*

[Course link](https://www.coursera.org/learn/pca-machine-learning/home)







